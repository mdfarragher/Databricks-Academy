{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "2d781abc-d3b7-4a81-858b-7a889a8bfdb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1727f835-99be-4a87-80da-7650e611560c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# Complex Transformations\n",
    "\n",
    "Querying tabular data stored in the data lakehouse with Spark SQL is easy, efficient, and fast.\n",
    "\n",
    "This gets more complicated as the data structure becomes less regular, when many tables need to be used in a single query, or when the shape of data needs to be changed dramatically. This notebook introduces a number of functions present in Spark SQL to help engineers complete even the most complicated transformations.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Use **`.`** and **`:`** syntax to query nested data\n",
    "- Parse JSON strings into structs\n",
    "- Flatten and unpack arrays and structs\n",
    "- Combine datasets using joins\n",
    "- Reshape data using pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77f6fc5-aa7e-464d-9c49-d2a9025564a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "## Run Setup\n",
    "\n",
    "The setup script will create the data and declare necessary values for the rest of this notebook to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a848adfa-434e-4b64-9265-19bff69a3011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-02.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7b2603f-4f52-403e-984f-5e76d9283252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Overview\n",
    "\n",
    "The **`events_raw`** table was registered against data representing a Kafka payload. In most cases, Kafka data will be binary-encoded JSON values. \n",
    "\n",
    "Let's cast the **`key`** and **`value`** as strings to view these values in a human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b86e7f6-0979-4231-a2b4-bd1bd62e5b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW events_strings AS \n",
    "SELECT string(key), string(value) FROM events_raw;\n",
    "\n",
    "SELECT * FROM events_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58316c85-b2b9-42ae-8809-f018fc978cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "events_stringsDF = (spark\n",
    "    .table(\"events_raw\")\n",
    "    .select(col(\"key\").cast(\"string\"), \n",
    "            col(\"value\").cast(\"string\"))\n",
    "    )\n",
    "display(events_stringsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ff7747-7311-4a86-a169-7685ee640ed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Manipulate Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9c8521-f9e2-4e67-968b-4653e376893a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Work with Nested Data\n",
    "\n",
    "The code cell below queries the converted strings to view an example JSON object without null fields (we'll need this for the next section).\n",
    "\n",
    "**NOTE:** Spark SQL has built-in functionality to directly interact with nested data stored as JSON strings or struct types.\n",
    "- Use **`:`** syntax in queries to access subfields in JSON strings\n",
    "- Use **`.`** syntax in queries to access subfields in struct types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97349c42-9958-496d-a4ee-5812a3925fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT * FROM events_strings WHERE value:event_name = \"finalize\" ORDER BY key LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55798b52-b6ff-4dba-a17b-3a3117016565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(events_stringsDF\n",
    "    .where(\"value:event_name = 'finalize'\")\n",
    "    .orderBy(\"key\")\n",
    "    .limit(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97587228-7fe5-4def-95c7-efabd8c1a575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's use the JSON string example above to derive the schema, then parse the entire JSON column into struct types.\n",
    "- **`schema_of_json()`** returns the schema derived from an example JSON string.\n",
    "- **`from_json()`** parses a column containing a JSON string into a struct type using the specified schema.\n",
    "\n",
    "After we unpack the JSON string to a struct type, let's unpack and flatten all struct fields into columns.\n",
    "- **`*`** unpacking can be used to flattens structs; **`col_name.*`** pulls out the subfields of **`col_name`** into their own columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163a01c4-d8aa-4752-acc2-12abab297021",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT schema_of_json('{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1075.5,\"total_item_quantity\":1,\"unique_items\":1},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879231210816,\"event_timestamp\":1593879335779563,\"geo\":{\"city\":\"Houston\",\"state\":\"TX\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_K\",\"item_name\":\"Standard King Mattress\",\"item_revenue_in_usd\":1075.5,\"price_in_usd\":1195.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593454417513109,\"user_id\":\"UA000000106116176\"}') AS schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6d5e2a-13c4-4b61-a4fd-4383c9795d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW parsed_events AS SELECT json.* FROM (\n",
    "SELECT from_json(value, 'STRUCT<device: STRING, ecommerce: STRUCT<purchase_revenue_in_usd: DOUBLE, total_item_quantity: BIGINT, unique_items: BIGINT>, event_name: STRING, event_previous_timestamp: BIGINT, event_timestamp: BIGINT, geo: STRUCT<city: STRING, state: STRING>, items: ARRAY<STRUCT<coupon: STRING, item_id: STRING, item_name: STRING, item_revenue_in_usd: DOUBLE, price_in_usd: DOUBLE, quantity: BIGINT>>, traffic_source: STRING, user_first_touch_timestamp: BIGINT, user_id: STRING>') AS json \n",
    "FROM events_strings);\n",
    "\n",
    "SELECT * FROM parsed_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42c8e766-3c2f-4421-9f00-0d83c998b3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import from_json, schema_of_json\n",
    "\n",
    "json_string = \"\"\"\n",
    "{\"device\":\"Linux\",\"ecommerce\":{\"purchase_revenue_in_usd\":1047.6,\"total_item_quantity\":2,\"unique_items\":2},\"event_name\":\"finalize\",\"event_previous_timestamp\":1593879787820475,\"event_timestamp\":1593879948830076,\"geo\":{\"city\":\"Huntington Park\",\"state\":\"CA\"},\"items\":[{\"coupon\":\"NEWBED10\",\"item_id\":\"M_STAN_Q\",\"item_name\":\"Standard Queen Mattress\",\"item_revenue_in_usd\":940.5,\"price_in_usd\":1045.0,\"quantity\":1},{\"coupon\":\"NEWBED10\",\"item_id\":\"P_DOWN_S\",\"item_name\":\"Standard Down Pillow\",\"item_revenue_in_usd\":107.10000000000001,\"price_in_usd\":119.0,\"quantity\":1}],\"traffic_source\":\"email\",\"user_first_touch_timestamp\":1593583891412316,\"user_id\":\"UA000000106459577\"}\n",
    "\"\"\"\n",
    "parsed_eventsDF = (events_stringsDF\n",
    "    .select(from_json(\"value\", schema_of_json(json_string)).alias(\"json\"))\n",
    "    .select(\"json.*\")\n",
    ")\n",
    "\n",
    "display(parsed_eventsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb2976e-903a-4413-b9ef-30f2ae11e67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Manipulate Arrays\n",
    "\n",
    "Spark SQL has a number of functions for manipulating array data, including the following:\n",
    "- **`explode()`** separates the elements of an array into multiple rows; this creates a new row for each element.\n",
    "- **`size()`** provides a count for the number of elements in an array for each row.\n",
    "\n",
    "The code below explodes the **`items`** field (an array of structs) into multiple rows and shows events containing arrays with 3 or more items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785fd5d7-e290-46b5-8108-b1b4d0b929d5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"geo\":225},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762884537322}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW exploded_events AS\n",
    "SELECT *, explode(items) AS item\n",
    "FROM parsed_events;\n",
    "\n",
    "SELECT * FROM exploded_events WHERE size(items) > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da3f2b1-1915-46bd-8142-f62269713aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import explode, size\n",
    "\n",
    "exploded_eventsDF = (parsed_eventsDF\n",
    "    .withColumn(\"item\", explode(\"items\"))\n",
    ")\n",
    "\n",
    "display(exploded_eventsDF.where(size(\"items\") > 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c65410-1c05-497d-8510-127f6864bac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE exploded_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6e8907-61f9-42de-868a-709a5f14e4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The code below combines array transformations to create a table that shows the unique collection of actions and the items in a user's cart.\n",
    "- **`collect_set()`** collects unique values for a field, including fields within arrays.\n",
    "- **`flatten()`** combines multiple arrays into a single array.\n",
    "- **`array_distinct()`** removes duplicate elements from an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a3a6a09-245b-4e36-8044-d16a4acc62f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT user_id,\n",
    "  collect_set(event_name) AS event_history,\n",
    "  array_distinct(flatten(collect_set(items.item_id))) AS cart_history\n",
    "FROM exploded_events\n",
    "GROUP BY user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48e1eb8b-5288-4ff3-b96d-4e3d44bfe320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql.functions import array_distinct, collect_set, flatten\n",
    "\n",
    "display(exploded_eventsDF\n",
    "    .groupby(\"user_id\")\n",
    "    .agg(collect_set(\"event_name\").alias(\"event_history\"),\n",
    "            array_distinct(flatten(collect_set(\"items.item_id\"))).alias(\"cart_history\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74a38d5-657c-4a19-aa91-a9ebef7da5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "## Combine and Reshape Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6310fdf7-4590-4b2a-84af-3297d20fd797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " \n",
    "### Join Tables\n",
    "\n",
    "Spark SQL supports standard **`JOIN`** operations (inner, outer, left, right, anti, cross, semi).  \n",
    "Here we join the exploded events dataset with a lookup table to grab the standard printed item name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33978705-9eff-4fab-8cdc-31bf45dbfdad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TEMP VIEW item_purchases AS\n",
    "\n",
    "SELECT * \n",
    "FROM (SELECT *, explode(items) AS item FROM sales) a\n",
    "INNER JOIN item_lookup b\n",
    "ON a.item.item_id = b.item_id;\n",
    "\n",
    "SELECT * FROM item_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e010679b-c64f-4d45-a318-823f5343e10b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "exploded_salesDF = (spark\n",
    "    .table(\"sales\")\n",
    "    .withColumn(\"item\", explode(\"items\"))\n",
    ")\n",
    "\n",
    "itemsDF = spark.table(\"item_lookup\")\n",
    "\n",
    "item_purchasesDF = (exploded_salesDF\n",
    "    .join(itemsDF, exploded_salesDF.item.item_id == itemsDF.item_id)\n",
    ")\n",
    "\n",
    "display(item_purchasesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dfdc6fc-e773-4de7-a2f9-941a8aa23cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pivot Tables\n",
    "\n",
    "We can use **`PIVOT`** to view data from different perspectives by rotating unique values in a specified pivot column into multiple columns based on an aggregate function.\n",
    "- The **`PIVOT`** clause follows the table name or subquery specified in a **`FROM`** clause, which is the input for the pivot table.\n",
    "- Unique values in the pivot column are grouped and aggregated using the provided aggregate expression, creating a separate column for each unique value in the resulting pivot table.\n",
    "\n",
    "The following code cell uses **`PIVOT`** to flatten out the item purchase information contained in several fields derived from the **`sales`** dataset. This flattened data format can be useful for dashboarding, but also useful for applying machine learning algorithms for inference or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f65be0-8585-4986-9693-7a443efa2322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM item_purchases\n",
    "PIVOT (\n",
    "  sum(item.quantity) FOR item_id IN (\n",
    "    'P_FOAM_K',\n",
    "    'M_STAN_Q',\n",
    "    'P_FOAM_S',\n",
    "    'M_PREM_Q',\n",
    "    'M_STAN_F',\n",
    "    'M_STAN_T',\n",
    "    'M_PREM_K',\n",
    "    'M_PREM_F',\n",
    "    'M_STAN_K',\n",
    "    'M_PREM_T',\n",
    "    'P_DOWN_S',\n",
    "    'P_DOWN_K')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6304b63b-ae63-4565-a543-19495023495b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "transactionsDF = (item_purchasesDF\n",
    "    .groupBy(\"order_id\", \n",
    "        \"email\",\n",
    "        \"transaction_timestamp\", \n",
    "        \"total_item_quantity\", \n",
    "        \"purchase_revenue_in_usd\", \n",
    "        \"unique_items\",\n",
    "        \"items\",\n",
    "        \"item\",\n",
    "        \"name\",\n",
    "        \"price\")\n",
    "    .pivot(\"item_id\")\n",
    "    .sum(\"item.quantity\")\n",
    ")\n",
    "display(transactionsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637125bd-c548-4187-ac81-4faf5a994c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    " \n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f878d5a-6826-4492-a696-c5d926adf4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "991063c5-75f1-4258-a6f0-eb1a14d214f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE 2.5 - Complex Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
